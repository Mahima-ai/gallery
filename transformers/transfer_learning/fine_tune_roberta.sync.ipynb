{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7fc87b",
   "metadata": {},
   "source": [
    "# Transfer learning with Transformers ü§ù BentoML\n",
    "\n",
    "[Source](https://github.com/bentoml/gallery/tree/main/transformers/roberta_text_classification/transfer_learning/fine_tune_roberta.sync.ipynb) | [nbviewer](https://nbviewer.org/github/bentoml/gallery/blob/main/transformers/roberta_text_classification/transfer_learning/fine_tune_roberta.sync.ipynb) | [Colab](https://colab.research.google.com/github/bentoml/gallery/blob/main/transformers/roberta_text_classification/transfer_learning/fine_tune_roberta.sync.ipynb)\n",
    "\n",
    "In this Jupyter notebook file, we will do transfer learning a version of [distilroberta-base](https://huggingface.co/distilroberta-base) for emotion detection (sentiment analysis) from text.\n",
    "\n",
    "<b>Stack: Transformers(<i>PyTorch backend</i>) - BentoML </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e9829d",
   "metadata": {},
   "source": [
    "## Import pretrained model with BentoML\n",
    "Users can easily import the our fine tune model with `bentoml.models.import_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import torch\n",
    "tag = bentoml.models.import_model(\"./exported\")\n",
    "model, tokenizer = bentoml.transformers.load(tag, return_config=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac81e10",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> If you just want to use the provided model, stop here and go back to [README.md](../README.md) for next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535ab5e",
   "metadata": {},
   "source": [
    "## Fine-tuning for multi-class sentiment analysis in a different domain\n",
    "In this section, we will fine tune a version of [distilroberta-base](https://huggingface.co/distilroberta-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8852a00",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d107740",
   "metadata": {},
   "source": [
    "### Setup pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python import_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93eea48",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "from config import (\n",
    "    BENTOML_FINETUNE_NAME,\n",
    "    MODEL,\n",
    "    NUM_LABELS,\n",
    "    NUM_EPOCHS,\n",
    "    BATCH_SIZE,\n",
    "    LR,\n",
    "    WDECAY,\n",
    ")\n",
    "from transformers.trainer_utils import set_seed\n",
    "from datasets.load import load_dataset\n",
    "\n",
    "torch.set_num_threads(psutil.cpu_count())\n",
    "set_seed(420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63605bd1",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "We will use [emotion](https://huggingface.co/datasets/emotion) via [huggingface/datasets](https://huggingface.co/docs/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ad59f",
   "metadata": {},
   "source": [
    "We will load tokenizer from imported model from HuggingFace hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b75ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tags.log\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tag = f.readline().strip(\"\\n\")\n",
    "_, tokenizer = bentoml.transformers.load(tag, return_config=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c5967",
   "metadata": {},
   "source": [
    "The following `preprocess_function` will [map](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map)\n",
    "all given text in the dataset to a tokenized version. We can then later use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "tokenized_emotion = emotion.map(preprocess_function, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4c178",
   "metadata": {},
   "source": [
    "We will use f1 and recall as our metrics for the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./fine_tune/\", num_labels=NUM_LABELS, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_emotion.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_emotion[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97377b9",
   "metadata": {},
   "source": [
    "Setup training arguments as well as `transformers.Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = len(tokenized_emotion[\"train\"]) // BATCH_SIZE\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    weight_decay=WDECAY,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tokenized_emotion[\"train\"],\n",
    "    eval_dataset=tokenized_emotion[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1543db4",
   "metadata": {},
   "source": [
    "Transformers provided an easy way to fine-tune given models with `Trainer` API. Simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_output = trainer.train()\n",
    "trainer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f20c61",
   "metadata": {},
   "source": [
    "### Evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd933b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a090680",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(tokenized_emotion[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d846c",
   "metadata": {},
   "source": [
    "### Save our fine-tune model to BentoML modelstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8551af",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = results.update({\"transfer-learning\": True})\n",
    "tag = bentoml.transformers.save(BENTOML_FINETUNE_NAME, model, tokenizer=tokenizer, metadata=metadata)\n",
    "bentoml.models.export_model(tag, \"./exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
