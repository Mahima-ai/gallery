{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bf0361",
   "metadata": {},
   "source": [
    "# Transfer learning with Transformers ü§ù BentoML\n",
    "\n",
    "In this Jupyter notebook file, we will perform transfer learning with a fine-tune version trained from [our fine-tune guide](./fine_tune_roberta.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd478c60",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78490eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71da52b",
   "metadata": {},
   "source": [
    "## Import fine-tune model from BentoML modelstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49e10de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:08:14] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span>boot<span style=\"font-weight: bold\">]</span> JAX version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, Flax version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> available.                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:08:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0mboot\u001b[1m]\u001b[0m JAX version \u001b[1;36m0.2\u001b[0m.\u001b[1;36m28\u001b[0m, Flax version \u001b[1;36m0.4\u001b[0m.\u001b[1;36m0\u001b[0m available.                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bentoml\n",
    "import transformers\n",
    "\n",
    "TASKS = \"text-classification\"\n",
    "FT_TAG = \"drobert_ft:latest\"\n",
    "\n",
    "config, model, tokenizer = bentoml.transformers.load(FT_TAG, return_config=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86924b",
   "metadata": {},
   "source": [
    "One can load the aboved `model`, `tokenizer`, and `config` to a `text-classification` pipeline to test with offline serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e23ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aarnphm/mambaforge/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'sadness', 'score': 0.059696026146411896},\n",
       "  {'label': 'joy', 'score': 0.08176055550575256},\n",
       "  {'label': 'love', 'score': 0.8277080059051514},\n",
       "  {'label': 'anger', 'score': 0.017906058579683304},\n",
       "  {'label': 'fear', 'score': 0.007731563411653042},\n",
       "  {'label': 'surprise', 'score': 0.00519789382815361}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipeline = transformers.pipeline(TASKS, model=model, tokenizer=tokenizer, config=config, return_all_scores=True)  # type: ignore\n",
    "clf_pipeline(\"I love you so much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc27a4",
   "metadata": {},
   "source": [
    "One can also Verify this model in a runner for offline serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = bentoml.transformers.load_runner(FT_TAG, tasks=TASKS, return_all_scores=True)\n",
    "\n",
    "runner.run_batch([\"Hello World\", \"I love you\", \"I hate you\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0661140",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> using `run_batch` should only be used for offline serving.\n",
    "\n",
    "In the context of a BentoML Service, `run_batch` or `async_run_batch` shouldn't\n",
    "be used as the BentoML's dynamic batching is <b>NOT ENABLED</b>.\n",
    "\n",
    "If users want to utilize multiple inputs for a request, BentoML support _composing inference graph_, which will be demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58877ea6",
   "metadata": {},
   "source": [
    "## Create a BentoML service\n",
    "<b>NOTE:</b> using `%%writefile` here because `bentoml.Service` instance must be created in a separate .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d83ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile service.py\n",
    "import re\n",
    "import typing as t\n",
    "import asyncio\n",
    "import unicodedata\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import JSON\n",
    "from bentoml.io import Text\n",
    "\n",
    "MODEL_NAME = \"roberta_text_classification\"\n",
    "TASKS = \"text-classification\"\n",
    "\n",
    "clf_runner = bentoml.transformers.load_runner(MODEL_NAME, tasks=TASKS)\n",
    "\n",
    "all_runner = bentoml.transformers.load_runner(\n",
    "    MODEL_NAME, name=\"all_score_runner\", tasks=TASKS, return_all_scores=True\n",
    ")\n",
    "\n",
    "svc = bentoml.Service(name=\"pretrained_clf\", runners=[clf_runner, all_runner])\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    s = \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", s.lower().strip())\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess(sentence: t.Dict[str, t.List[str]]) -> t.Dict[str, t.List[str]]:\n",
    "    assert \"text\" in sentence, \"Given JSON does not contain `text` field\"\n",
    "    if not isinstance(sentence[\"text\"], list):\n",
    "        sentence[\"text\"] = [sentence[\"text\"]]\n",
    "    return {k: [normalize(s) for s in v] for k, v in sentence.items()}\n",
    "\n",
    "\n",
    "def convert_result(res) -> t.Dict[str, t.Any]:\n",
    "    if isinstance(res, list):\n",
    "        return {l[\"label\"]: l[\"score\"] for l in res}\n",
    "    return {res[\"label\"]: res[\"score\"]}\n",
    "\n",
    "\n",
    "def postprocess(\n",
    "    inputs: t.Dict[str, t.List[str]], outputs: t.List[t.Dict[str, t.Any]]\n",
    ") -> t.Dict[int, t.Dict[str, t.Union[str, float]]]:\n",
    "    return {\n",
    "        i: {\"input\": sent, **convert_result(pred)}\n",
    "        for i, (sent, pred) in enumerate(zip(inputs[\"text\"], outputs))\n",
    "    }\n",
    "\n",
    "\n",
    "@svc.api(input=Text(), output=JSON())\n",
    "async def sentiment(sentence: str) -> t.Dict[str, t.Any]:\n",
    "    res = await clf_runner.async_run(sentence)\n",
    "    return {\"input\": sentence, \"label\": res[\"label\"]}\n",
    "\n",
    "\n",
    "@svc.api(input=JSON(), output=JSON())\n",
    "async def batch_sentiment(\n",
    "    sentences: t.Dict[str, t.List[str]]\n",
    ") -> t.Dict[int, t.Dict[str, t.Union[str, float]]]:\n",
    "    processed = preprocess(sentences)\n",
    "    outputs = await asyncio.gather(\n",
    "        *[clf_runner.async_run(s) for s in processed[\"text\"]]\n",
    "    )\n",
    "    return postprocess(processed, outputs)  # type: ignore\n",
    "\n",
    "\n",
    "@svc.api(input=JSON(), output=JSON())\n",
    "async def batch_all_scores(\n",
    "    sentences: t.Dict[str, t.List[str]]\n",
    ") -> t.Dict[int, t.Dict[str, t.Union[str, float]]]:\n",
    "    processed = preprocess(sentences)\n",
    "    outputs = await asyncio.gather(\n",
    "        *[all_runner.async_run(s) for s in processed[\"text\"]]\n",
    "    )\n",
    "    return postprocess(processed, outputs)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd77fc3",
   "metadata": {},
   "source": [
    "\n",
    "We defined two separate endpoints `/batch_sentiment` and `batch_all_scores` which both creates an inference graph to make use of BentoML's dynamic batching.\n",
    "\n",
    "We also create `/sentiment` endpoints which accept a single sentence as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586f5bd",
   "metadata": {},
   "source": [
    "\n",
    "Start a service with reload enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve service:svc --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb147f76",
   "metadata": {},
   "source": [
    "With the `--reload` flag, the API server will automatically restart when the source file `service.py` is being updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c257b",
   "metadata": {},
   "source": [
    "One can then navigate to `127.0.0.1:3000` and interact with Swagger UI.\n",
    "One can also verify the endpoints locally with `curl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf540e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://localhost:3000/batch_sentiment\" \\\n",
    "      -H \"accept: application/json\" \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d \"{\\\"text\\\":[\\\"I love you with all my hearts :)\\\",\\\"Our path diverges\\\"]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8777a529",
   "metadata": {},
   "source": [
    "We can also do a simple local benchmark with [locust](https://locust.io/):\n",
    "```bash\n",
    "locust --headless -u 100 -r 1000 --run-time 2m --host http://127.0.0.1:3000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860fd29",
   "metadata": {},
   "source": [
    "## Build a Bento for deployment\n",
    "\n",
    "A `bentofile.yaml` can be created to create a Bento with `bentoml build` in the current directory:\n",
    "```yaml\n",
    "service: \"service:svc\"\n",
    "description: \"file: ./README.md\"\n",
    "labels:\n",
    "  owner: bentoml-team\n",
    "  stage: demo\n",
    "include:\n",
    "- \"*.py\"\n",
    "exclude:\n",
    "- \"locustfile.py\"\n",
    "- \"tests/\"\n",
    "- \"*.ipynb\"\n",
    "python:\n",
    "  lock_packages: false\n",
    "  packages:\n",
    "    - -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "    - torch==1.10.2+cpu\n",
    "    - git+https://github.com/huggingface/transformers\n",
    "    - datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa86e7",
   "metadata": {},
   "source": [
    "Build a bento with `bentoml build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8d161",
   "metadata": {},
   "source": [
    "This Bento now can be served with `--production`:\n",
    "```bash\n",
    "bentoml serve pretrained_clf:latest --production\n",
    "```\n",
    "\n",
    "## Containerize a Bento\n",
    "\n",
    "Make sure Docker and daemon is running, then `bentoml containerize` will build\n",
    "a docker image for the model server aboved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml containerize pretrained_clf:latest\n",
    "# an example docker tag: pretrained_clf:zt4vvsurw63thgxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495435b",
   "metadata": {},
   "source": [
    "Test out the newly built docker image:\n",
    "```bash\n",
    "docker run -p 3000:3000 pretrained_clf:zt4vvsurw63thgxi\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
