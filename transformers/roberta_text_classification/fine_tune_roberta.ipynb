{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a682ea0b",
   "metadata": {},
   "source": [
    "# Fine-tuning Roberta for emotion classification\n",
    "\n",
    "[Source](https://github.com/bentoml/gallery/blob/main/transformers/roberta_text_classification).\n",
    "Try it out on [Colab](https://colab.research.google.com/github/bentoml/gallery/blob/main/transformers/roberta_text_classification/fine_tune_roberta.ipynb).\n",
    "\n",
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad00863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45393b74",
   "metadata": {},
   "source": [
    "## Fine-tune a Roberta model\n",
    "\n",
    "First let's define our [dataset](https://huggingface.co/datasets/sentiment140/viewer/sentiment140/test) using [huggingface/datasets](https://github.com/huggingface/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b007d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "import transformers\n",
    "\n",
    "from config import MODEL_NAME, MODEL\n",
    "from datasets.load import load_dataset\n",
    "from datasets import ClassLabel, Value\n",
    "\n",
    "transformers.set_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854475b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8220be6b78a4b979332bfb384b29a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625d1f2c96b444ebab81107807e5bcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /Users/aarnphm/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c30803511c54528ae2ae8b117658ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec6983765114ca9b73b42d263876370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119de7fe851e47c08d7a41e633b6cf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /Users/aarnphm/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f2a4e5ebdc41c4a95554b4edbb8ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion = load_dataset('emotion')\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caeff07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:50:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> JAX version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, Flax version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> available.                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:50:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m JAX version \u001b[1;36m0.2\u001b[0m.\u001b[1;36m28\u001b[0m, Flax version \u001b[1;36m0.4\u001b[0m.\u001b[1;36m0\u001b[0m available.                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> BentoML won't support loading pipeline if users decide to save pipeline  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         with `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">save</span><span style=\"font-weight: bold\">()</span>`.                                                           \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         Since `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">load</span><span style=\"font-weight: bold\">()</span>` will always return model, and tokenizer. Users can easily \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         create a new pipeline:                                                   \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>             import bentoml                                                       \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>             import transformers                                                  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>             model, tokenizer = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">bentoml.transformers.load</span><span style=\"font-weight: bold\">(</span>tag<span style=\"font-weight: bold\">)</span>                    \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>             pipe = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">transformers.pipeline</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'text-classification'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080\">model</span>,     \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer</span>=<span style=\"color: #800080; text-decoration-color: #800080\">tokenizer</span><span style=\"font-weight: bold\">)</span>                                                     \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m BentoML won't support loading pipeline if users decide to save pipeline  \n",
       "\u001b[2;36m           \u001b[0m         with `\u001b[1;35msave\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m`.                                                           \n",
       "\u001b[2;36m           \u001b[0m         Since `\u001b[1;35mload\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` will always return model, and tokenizer. Users can easily \n",
       "\u001b[2;36m           \u001b[0m         create a new pipeline:                                                   \n",
       "\u001b[2;36m           \u001b[0m             import bentoml                                                       \n",
       "\u001b[2;36m           \u001b[0m             import transformers                                                  \n",
       "\u001b[2;36m           \u001b[0m                                                                                  \n",
       "\u001b[2;36m           \u001b[0m             model, tokenizer = \u001b[1;35mbentoml.transformers.load\u001b[0m\u001b[1m(\u001b[0mtag\u001b[1m)\u001b[0m                    \n",
       "\u001b[2;36m           \u001b[0m             pipe = \u001b[1;35mtransformers.pipeline\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'text-classification'\u001b[0m, \u001b[33mmodel\u001b[0m=\u001b[35mmodel\u001b[0m,     \n",
       "\u001b[2;36m           \u001b[0m         \u001b[33mtokenizer\u001b[0m=\u001b[35mtokenizer\u001b[0m\u001b[1m)\u001b[0m                                                     \n",
       "\u001b[2;36m           \u001b[0m                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = bentoml.transformers.load(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce2b102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 6\n",
    "NUM_EPOCHS = 1\n",
    "NUM_EXAMPLES = 400\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "LR = 2e-5\n",
    "WDECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c6efe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6a18eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f4e6305dc242099211ff0802e62532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d2d422a39b4fc98adb1dc0844311e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3804e967584404a45f30db66ea75bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_emotion = emotion.map(preprocess_function, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4068f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58fef0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74a4c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "updated_model_head = transformers.AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=NUM_LABELS, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdccaddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_emotion['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c308725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_emotion.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_emotion['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80aa413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "logging_steps = len(tokenized_emotion['train'])//BATCH_SIZE\n",
    "training_args = transformers.TrainingArguments(output_dir='results',\n",
    "                                            num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                                            learning_rate = LR,\n",
    "                                            per_device_train_batch_size=BATCH_SIZE,\n",
    "                                            per_device_eval_batch_size=BATCH_SIZE,\n",
    "                                            load_best_model_at_end=True,\n",
    "                                            metric_for_best_model='f1',\n",
    "                                            weight_decay=WDECAY,\n",
    "                                            save_strategy='epoch',\n",
    "                                            evaluation_strategy='epoch',\n",
    "                                            disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afe26a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/aarnphm/mambaforge/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 126/1250 26:10 < 3:57:15, 0.08 it/s, Epoch 0.50/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.280883</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.869116</td>\n",
       "      <td>0.862759</td>\n",
       "      <td>0.879081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(model\u001b[38;5;241m=\u001b[39mupdated_model_head, \n\u001b[1;32m      2\u001b[0m                                args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      3\u001b[0m                                compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      4\u001b[0m                                train_dataset\u001b[38;5;241m=\u001b[39mtokenized_emotion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m                                eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_emotion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/trainer.py:1398\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1401\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1404\u001b[0m ):\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/trainer.py:2000\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1998\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2000\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(model=updated_model_head, \n",
    "                               args=training_args,\n",
    "                               compute_metrics=compute_metrics,\n",
    "                               train_dataset=tokenized_emotion['train'],\n",
    "                               eval_dataset=tokenized_emotion['validation'])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec0f8dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28088268637657166,\n",
       " 'eval_accuracy': 0.9045,\n",
       " 'eval_f1': 0.8691157729199627,\n",
       " 'eval_precision': 0.8627589064134121,\n",
       " 'eval_recall': 0.879081087747385}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1abce24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.28088268637657166,\n",
       " 'test_accuracy': 0.9045,\n",
       " 'test_f1': 0.8691157729199627,\n",
       " 'test_precision': 0.8627589064134121,\n",
       " 'test_recall': 0.879081087747385,\n",
       " 'test_runtime': 76.4113,\n",
       " 'test_samples_per_second': 26.174,\n",
       " 'test_steps_per_second': 0.419}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output = trainer.predict(tokenized_emotion[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcc87435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:03:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> JAX version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span>, Flax version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> available.                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:03:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m JAX version \u001b[1;36m0.2\u001b[0m.\u001b[1;36m28\u001b[0m, Flax version \u001b[1;36m0.4\u001b[0m.\u001b[1;36m0\u001b[0m available.                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /var/folders/b1/90qqtv1n53v9tdl15l0phky40000gn/T/tmp0x4wbh0nbentoml_model_roberta_text_classification/config.json\n",
      "Model weights saved in /var/folders/b1/90qqtv1n53v9tdl15l0phky40000gn/T/tmp0x4wbh0nbentoml_model_roberta_text_classification/pytorch_model.bin\n",
      "tokenizer config file saved in /var/folders/b1/90qqtv1n53v9tdl15l0phky40000gn/T/tmp0x4wbh0nbentoml_model_roberta_text_classification/tokenizer_config.json\n",
      "Special tokens file saved in /var/folders/b1/90qqtv1n53v9tdl15l0phky40000gn/T/tmp0x4wbh0nbentoml_model_roberta_text_classification/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Successfully saved                                                       \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Model</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">tag</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"roberta_text_classification:fa3yeten7oi4zgxi\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"/Users/aa</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         <span style=\"color: #008000; text-decoration-color: #008000\">rnphm/bentoml/models/roberta_text_classification/fa3yeten7oi4zgxi/\"</span><span style=\"font-weight: bold\">)</span>     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Successfully saved                                                       \n",
       "\u001b[2;36m           \u001b[0m         \u001b[1;35mModel\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtag\u001b[0m=\u001b[32m\"roberta_text_classification\u001b[0m\u001b[32m:fa3yeten7oi4zgxi\"\u001b[0m, \u001b[33mpath\u001b[0m=\u001b[32m\"/Users/aa\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m         \u001b[32mrnphm/bentoml/models/roberta_text_classification/fa3yeten7oi4zgxi/\"\u001b[0m\u001b[1m)\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metadata = results.update({\"transfer-learning\": True})\n",
    "tag = bentoml.transformers.save(MODEL_NAME, updated_model_head, tokenizer=tokenizer, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f05fae93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta_text_classification:fa3yeten7oi4zgxi'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{tag.name}:{tag.version}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "transformers_roberta_text_classification_demo.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
